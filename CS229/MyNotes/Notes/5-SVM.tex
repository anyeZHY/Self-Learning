\section{Support Vector Machines}
SVMs are among the best (and many believe are indeed the best) “off-the-shelf” supervised learning algorithms.
So, be \textbf{self-motivated} in this section.
\subsection{Hard-SVM}
\textbf{Hard-SVM} is the learning rule in which we return an ERM hyperplane that separates the training set with the largest possible margin. The Hard-SVM rule is
\[
	\argmax_{(w,b):\norm{w}=1}\min_{i\in[m]}\abs{w^Tx^i+b}
	\ \ \ 
	\text{s.t. }\forall i, y^i\qty(w^Tx^i+b)\ge 1
.\]
Equivalently,
\begin{equation}
	\argmax_{(w,b):\norm{w}=1}\min_{i\in[m]}y^i\qty(w^Tx^i+b)
	\label{naive_svm}
\end{equation}
Next, we give another equivalent formulation of the Hard-SVM rule as a quadratic optimization problem.\footnote{A quadratic optimization problem is an optimization problem in which the objective is a convex quadratic function and the constraints are linear inequalities.}
\begin{md}
	Input: $(x^1,^1),\cdots,(x^m,y^m)$

	Solve:
	\begin{equation}
		(w_0,b_0)=\argmin_{(w,b)}\frac{1}{2}\norm{w}^2
		\ \ \ 
		\text{s.t. }\forall i, y^i\qty(w^Tx^i+b)\ge 1.
		\label{svm}
	\end{equation}

	Output: $\hat{w}=\flatfrac{w_0}{\norm{w_0}}$, $\hat{b}=\flatfrac{b_0}{\norm{w_0}}$
\end{md}
\begin{lemma}
	The output of Hard-SVM is a solution of Equation (\ref{naive_svm}).
\end{lemma}
\begin{prf}
	Let ($w_1$, $b_1$) be a solution of Equation (\ref{naive_svm}) and $\gamma_1=\min_{i\in[m]}y_i(w_1^{T}x^i+b_1)$. Then we have
	\[
		y^i\qty(\frac{w_1}{\gamma_1}^Tx^i+\frac{b_1}{\gamma_1})\ge 1
	.\] 
	Hence $\norm{w_0}\le \norm{\flatfrac{w_1}{\gamma_1}}=\flatfrac{1}{\gamma^*}$.
	It follows that for all $i$,
	 \[
		 y^i(\hat{w}^Tx^i+\hat{b})\ge \frac{1}{\norm{w_0}}\ge \gamma_1
	.\] 
	Since $\norm{\hat{w}} = 1$ we obtain that $(\hat{w},\hat{b})$ is an optimal solution of Equation (\ref{naive_svm}).
\end{prf}
\subsubsection{The Sample Complexity of Hard-SVM*}
\begin{defi}[Separability]
	Let $\+D$ be a distribution over $\RR^d \times \{\pm1\}$.
	We say that $\+D$ is separable with a $(\gamma,\rho)$-margin if there exists $(w^*,b^*)$ such that $\norm{w^*} = 1$ and such that with probability 1 over the choice of $(x, y) \sim \+D$ we have that $y(w^{*T}x+ b^*)\ge \gamma$ and $\norm{x}\le  \rho$. 
\end{defi}
\begin{thm}
	Let $\+D$ be a distribution over $\RR^d\times\{\pm1\}$ that satisfies the $(\gamma,\rho)$-separability with margin assumption using a homogenous halfspace.
	Then, with probability of at least $1 - \delta$ over the choice of a training set of size m, the 0-1 error of the output of Hard-SVM is at most
	\[
		\sqrt{\frac{4(\flatfrac{\rho)}{\gamma^2}}{m}}+\sqrt{\frac{2\log(\flatfrac{2}{\delta})}{m}}
	.\] 
	
\end{thm}
\subsection{Soft-SVM and Norm Regularization}
\begin{md}
	Input: $(x^1,^1),\cdots,(x^m,y^m)$

	Parameter: $\lambda>0$

	Solve:
	\[
		\begin{aligned}
			& \min_{w,b,\xi}\qty(\lambda\norm{w}^2+\frac{1}{m}\sum_{i=1}^m\xi_i)
			\\
			&\text{s.t. }\forall i,\ y^i\qty(w^Tx^i+b)\ge 1-\xi_i \text{ and }
			\xi_i\ge 0
		\end{aligned}
	.\]

	Output: $w,b$
\end{md}
\begin{defi}[hinge loss]
	\[
		l^{\mathrm{hinge}}((w,b),(x,y))=\max\qty{0,1-yw^Tx+b}
	.\]
\end{defi}
Now we just need to optimize $\lambda\norm{w}^2+\+L^{\mathrm{hinge}}(w,b)$.
\subsection{Duality}
The lagrangian for EQ.\ref{svm} is:
\[
	\+L(w,b,\alpha)=\frac{1}{2}\norm{w}^2-\sum_{i=1}^n\alpha_i\qty[y^i\qty(w^Tx^i+b)-1]
.\] 


\subsection{Implementing Soft-SVM Using SGD}
\begin{algorithm}
	\caption{SGD for Solving Soft-SVM}
	\begin{algorithmic}
		\STATE $\*\theta=\*0$
		\FOR{$i=1,\cdots,T$} 
		\STATE $w^{(t)}=\flatfrac{1}{\lambda t}\times\*\theta$
		\STATE Choose $i$ uniformly at random for $[m]$
		\IF{$y_iw^Tx^i<1$}
		\STATE $\*\theta\gets\*\theta+y^ix^i$
		\ENDIF
		\ENDFOR
		\RETURN $\sum_{t=1}^T w^{(t)}/T$
	\end{algorithmic}
\end{algorithm}
